{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5CZRUUPnEO/aJCC93A0TR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viniciusp67/Consultor-de-Direito-Eleitoral/blob/main/teste.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtTYYm7c-8jI",
        "outputId": "ce643e49-ddf1-4090-e7b8-ec947c72b526",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "enc.encode('realizando testes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-Upxtp1_Nrz",
        "outputId": "a0f0123c-c2d3-4d34-b2f1-825aa7f1804b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5305, 528, 25440, 1332, 274]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto = 'A soberania popular será exercida pelo sufrágio universal e pelo voto direto e secreto, com valor igual para todos...'\n",
        "alfabeto = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "caracteres = sorted(list(set(texto + alfabeto)))\n",
        "print(caracteres)\n",
        "print('Tamanho do seu vocabulário >> ', len(caracteres))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfdyN-RbBXIB",
        "outputId": "e3ad683e-612d-420f-8a30-076d99d2a412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', ',', '.', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'á']\n",
            "Tamanho do seu vocabulário >>  56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "letraParaIndex = { lt:i for i,lt in enumerate(caracteres) }\n",
        "IndexParaLetra = { i:lt for i,lt in enumerate(caracteres) }\n",
        "\n",
        "encode = lambda s: [letraParaIndex[c] for c in s]\n",
        "decode = lambda l: ''.join([IndexParaLetra[i] for i in l])"
      ],
      "metadata": {
        "id": "eos4bqHiBPkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode('A soberania popular será exercida pelo sufrágio universal e pelo voto direto e secreto, com valor igual para todos...'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc6TvWE2_RFr",
        "outputId": "ff25c824-7797-40a7-c366-699dc58caca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 0, 47, 43, 30, 33, 46, 29, 42, 37, 29, 0, 44, 43, 44, 49, 40, 29, 46, 0, 47, 33, 46, 55, 0, 33, 52, 33, 46, 31, 37, 32, 29, 0, 44, 33, 40, 43, 0, 47, 49, 34, 46, 55, 35, 37, 43, 0, 49, 42, 37, 50, 33, 46, 47, 29, 40, 0, 33, 0, 44, 33, 40, 43, 0, 50, 43, 48, 43, 0, 32, 37, 46, 33, 48, 43, 0, 33, 0, 47, 33, 31, 46, 33, 48, 43, 1, 0, 31, 43, 41, 0, 50, 29, 40, 43, 46, 0, 37, 35, 49, 29, 40, 0, 44, 29, 46, 29, 0, 48, 43, 32, 43, 47, 2, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto = encode('Compra de votos e outras formas de influenciar ilegalmente o eleitor')\n",
        "len(texto)\n",
        "print(texto)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz_mSDLU_75N",
        "outputId": "bea8d629-a7f4-4d56-deeb-913020ff2902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 43, 41, 44, 46, 29, 0, 32, 33, 0, 50, 43, 48, 43, 47, 0, 33, 0, 43, 49, 48, 46, 29, 47, 0, 34, 43, 46, 41, 29, 47, 0, 32, 33, 0, 37, 42, 34, 40, 49, 33, 42, 31, 37, 29, 46, 0, 37, 40, 33, 35, 29, 40, 41, 33, 42, 48, 33, 0, 43, 0, 33, 40, 33, 37, 48, 43, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(texto)-1):\n",
        "  x = texto[:i]\n",
        "  y = texto[i]\n",
        "  if x != []:\n",
        "    print(f'Quando os dados forem: {x} o alvo é {y}')"
      ],
      "metadata": {
        "id": "nN9bSGAxAVLV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "for i in range(len(texto)):\n",
        "  x = texto\n",
        "  y = texto.copy()\n",
        "  idx_mask = randint(0,len(texto)-1)\n",
        "  y[idx_mask] = '<mask>'\n",
        "  print(f'Quando os dados forem: {x} o alvo é {y}')"
      ],
      "metadata": {
        "id": "If6pachFAfDI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7rxmD66DAWUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = './sample_data/'\n",
        "dados_treino = 'Codigo_Eleitoral.txt'"
      ],
      "metadata": {
        "id": "k9OAMwjYAX_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train(files=[PATH+dados_treino], vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "    ])"
      ],
      "metadata": {
        "id": "lecmoJW0AoZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ./sample_data/RAW_MODEL\n",
        "!mkdir ./sample_data/RAW_MODEL\n",
        "tokenizer.save_model(PATH+'RAW_MODEL')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPXSvfUSBFXA",
        "outputId": "21b79a4e-703f-46c2-8ca0-8951b4a376df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove './sample_data/RAW_MODEL': No such file or directory\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./sample_data/RAW_MODEL/vocab.json', './sample_data/RAW_MODEL/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    PATH+'RAW_MODEL'+\"/vocab.json\",\n",
        "    PATH+'RAW_MODEL'+\"/merges.txt\",\n",
        ")\n",
        "\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "metadata": {
        "id": "olExQv9SBbN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer =  RobertaTokenizer.from_pretrained(PATH+'RAW_MODEL', max_len=512)"
      ],
      "metadata": {
        "id": "nQB0f1tFDRB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=512,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n",
        "\n",
        "from transformers import RobertaForMaskedLM\n",
        "model = RobertaForMaskedLM(config=config)"
      ],
      "metadata": {
        "id": "isQSftx6DfiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.num_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRxAie4fDksD",
        "outputId": "24ab4594-31e6-4d34-cfdd-0c20ebbbd64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83502880"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=PATH+dados_treino,\n",
        "    block_size=128,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYusT5mPDnSA",
        "outputId": "6c147d77-cc99-49c5-fbc1-1df3ee3bcaf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.examples[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k1sr9NNrJNNt",
        "outputId": "f26d02e6-d884-4841-f4e5-89097771ccde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input_ids': tensor([   0,  387,   18,  307,  284, 4725,  816,  594,  820, 1424, 4816,  262,\n",
              "          4849,  262, 2165,  263,  278,  997,  265, 2468, 1197,  448,  295,  386,\n",
              "          4269,  364,  265,  883,  263,  357, 2028,   18,    2])},\n",
              " {'input_ids': tensor([   0,  597,  587,   18,  478,  426,  659,  517,  962,  455, 1561,  355,\n",
              "           619, 5517, 1510,   18,    2])}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "EjKKb_N5DvE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=PATH+'RAW_MODEL',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=200,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "filF1g4SDxRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "LGjE2s95Dzrp",
        "outputId": "fe32dcfc-9595-44af-cc49-381fd2c329e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='7600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   3/7600 00:51 < 108:15:52, 0.02 it/s, Epoch 0.05/200]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}