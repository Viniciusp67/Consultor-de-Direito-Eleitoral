{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viniciusp67/Consultor-de-Direito-Eleitoral/blob/main/DistilGPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0D14DxylV40"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch accelerate tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carregar o dataset\n",
        "with open('codigo_eleitoral .txt', 'r', encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Dividir o texto em exemplos\n",
        "def split_text(data, chunk_size=1024):\n",
        "    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "examples = split_text(data)\n",
        "df = pd.DataFrame(examples, columns=['text'])\n"
      ],
      "metadata": {
        "id": "Ex9uF87wmIBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Converter o dataframe para um dataset do Huggingface\n",
        "dataset = Dataset.from_pandas(df)\n"
      ],
      "metadata": {
        "id": "CnBHOeX7mKAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "import torch.nn as nn\n",
        "\n",
        "# Carregar o tokenizer e o modelo DistilGPT2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Definindo a configuração personalizada\n",
        "config = GPT2Config.from_pretrained('distilgpt2', attn_pdrop=0.2, resid_pdrop=0.2, embd_pdrop=0.2)\n",
        "\n",
        "# Definindo o modelo personalizado com Dropout adicional\n",
        "class CustomGPT2Model(GPT2LMHeadModel):\n",
        "    def __init__(self, config):\n",
        "        super(CustomGPT2Model, self).__init__(config)\n",
        "        self.custom_dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, input_ids=None, past_key_values=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
        "        # Chamando o forward do GPT2LMHeadModel\n",
        "        outputs = super(CustomGPT2Model, self).forward(input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\n",
        "\n",
        "        # Aplicando Dropout personalizado aos logits\n",
        "        logits = outputs.logits\n",
        "        logits = self.custom_dropout(logits)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Carregar o modelo com a configuração personalizada\n",
        "model = CustomGPT2Model.from_pretrained('distilgpt2', config=config)\n"
      ],
      "metadata": {
        "id": "dsA91CXEmMAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=1024)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=4)  # Utilizar múltiplos processos para acelerar\n",
        "\n",
        "# Adicionar os rótulos ao dataset tokenizado\n",
        "def add_labels(examples):\n",
        "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
        "    return examples\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
        "\n",
        "# Dividir o dataset em treinamento e validação\n",
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = tokenized_dataset[\"train\"]\n",
        "eval_dataset = tokenized_dataset[\"test\"]\n"
      ],
      "metadata": {
        "id": "DD9D-KNXmNhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "n-xUPBhdmOyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=-1)\n",
        "    # Remover o padding e calcular o F1-Score\n",
        "    true_predictions = [p for p, l in zip(predictions.flatten(), labels.flatten()) if l != -100]\n",
        "    true_labels = [l for l in labels.flatten() if l != -100]\n",
        "    f1 = f1_score(true_labels, true_predictions, average=\"weighted\")\n",
        "    return {\"f1\": f1}"
      ],
      "metadata": {
        "id": "1W0G2T0EzmOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, IntervalStrategy\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=300,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy=IntervalStrategy.STEPS,\n",
        "    eval_steps=500,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"tensorboard\",\n",
        ")"
      ],
      "metadata": {
        "id": "WsmROC3tmRHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "UntbeF01bOG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "vJofaRUimSq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho da pasta onde você quer salvar o modelo no Google Drive\n",
        "output_dir = '/content/drive/My Drive/fine_tuned_distilgpt2'\n",
        "\n",
        "# Criar o diretório se não existir\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Salvar o modelo e o tokenizer\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "vBvOcxJ9bWYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Monitorar o Treinamento\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs"
      ],
      "metadata": {
        "id": "ZFxiOgAlbcAl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}